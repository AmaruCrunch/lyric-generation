{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pickle\n",
    "# load the word2vec model\n",
    "from gensim.models import KeyedVectors\n",
    "from tqdm import tqdm\n",
    "#from torch.utils.tensorboard import SummaryWriter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LyricDataset(Dataset):\n",
    "    def __init__(self, lyrics_dict, midi_dict, word2vec):\n",
    "        \"\"\"\n",
    "        lyrics_dict: Dictionary of {song: [list of tokens]}\n",
    "        midi_dict: Dictionary of {song: midi_vector}\n",
    "        get_word_embedding: Function to convert a word to its corresponding embedding\n",
    "        \"\"\"\n",
    "        # get keys that are in both dictionaries\n",
    "        common_keys = lyrics_dict.keys() & midi_dict.keys()\n",
    "\n",
    "        self.samples = []\n",
    "        for song in common_keys:\n",
    "            midi_vector = torch.tensor(midi_dict[song][0], dtype=torch.float32)  # MIDI vector for the song\n",
    "            lyrics = lyrics_dict[song]\n",
    "            for i in range(len(lyrics) - 1):  # Exclude last word for which there is no next word\n",
    "                current_word_embedding = torch.tensor(word2vec.get_vector(lyrics[i]), dtype=torch.float32)\n",
    "                next_word_index = torch.tensor(lyrics[i + 1], dtype=torch.long)\n",
    "                self.samples.append((current_word_embedding, midi_vector, next_word_index))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        current_word_embedding, midi_vector, next_word_index = self.samples[idx]\n",
    "        return current_word_embedding, midi_vector, next_word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load lyrics_dict.pkl\n",
    "\n",
    "with open('data/lyrics_dict.pkl', 'rb') as f:\n",
    "    lyrics_dict = pickle.load(f)\n",
    "\n",
    "# load midi_feature_vectors.pkl\n",
    "with open('data/midi_embeddings.pkl', 'rb') as f:\n",
    "    midi_embeddings = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load word2vec model\n",
    "word2vec = KeyedVectors.load_word2vec_format('models/GoogleNews-vectors-negative300.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load midi_autoencoder from torch\n",
    "input = midi_embeddings['no woman no cry the fugees']\n",
    "len(input[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset and dataloader\n",
    "lyric_dataset = LyricDataset(lyrics_dict, midi_embeddings, word2vec)\n",
    "#create a dataloader\n",
    "lyric_dataloader = DataLoader(lyric_dataset, batch_size=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NTPModel(nn.Module):\n",
    "    def __init__(self, word_embedding_dim=300, midi_embedding_dim=1024, hidden_dim=512, num_layers=2, vocab_size=3000000):\n",
    "        super(NTPModel, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Define the LSTM layer\n",
    "        self.lstm = nn.LSTM(input_size=word_embedding_dim + midi_embedding_dim,\n",
    "                            hidden_size=hidden_dim,\n",
    "                            num_layers=num_layers,\n",
    "                            batch_first=True)\n",
    "        \n",
    "        # Define the output layer\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, word_embeddings, midi_embeddings):\n",
    "        # Concatenate word embeddings and MIDI embeddings along the feature dimension\n",
    "        combined_embeddings = torch.cat((word_embeddings.squeeze(), midi_embeddings.squeeze()), dim=-1)\n",
    "        \n",
    "        # LSTM layer\n",
    "        lstm_out, _ = self.lstm(combined_embeddings)\n",
    "        \n",
    "        # Pass through the output layer\n",
    "        predictions = self.fc(lstm_out)\n",
    "        \n",
    "        return predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Assuming the model is already defined and instantiated\n",
    "model = NTPModel()\n",
    "model.to(device)  # Assuming you're using a GPU\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, epochs, device):\n",
    "    writer = SummaryWriter()  # For logging to TensorBoard\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in tqdm(range(epochs), desc=\"Epochs\"):\n",
    "        total_loss = 0\n",
    "        for batch_idx, (current_word_embedding, midi_vector, next_word_index) in enumerate(tqdm(dataloader, desc=\"Training\")):\n",
    "            current_word_embedding = current_word_embedding.to(device)\n",
    "            midi_vector = midi_vector.to(device)\n",
    "            next_word_index = next_word_index.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(current_word_embedding, midi_vector)\n",
    "            loss = criterion(output.view(-1, vocab_size), next_word_index.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Log to TensorBoard\n",
    "            writer.add_scalar('Loss/train', loss.item(), epoch * len(dataloader) + batch_idx)\n",
    "        \n",
    "        # Print loss every epoch\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {total_loss/len(dataloader)}')\n",
    "        \n",
    "    writer.flush()\n",
    "    writer.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SummaryWriter' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# train the model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlyric_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[14], line 2\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, dataloader, epochs, device)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(model, dataloader, epochs, device):\n\u001b[1;32m----> 2\u001b[0m     writer \u001b[38;5;241m=\u001b[39m \u001b[43mSummaryWriter\u001b[49m()  \u001b[38;5;66;03m# For logging to TensorBoard\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(epochs), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpochs\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'SummaryWriter' is not defined"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "train(model, lyric_dataloader, 1, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "logdeep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
