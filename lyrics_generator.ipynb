{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "# load the word2vec model\n",
    "from gensim.models import KeyedVectors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LyricGeneratorModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=300, midi_vector_size=100, hidden_dim=512, num_layers=2):\n",
    "        super(LyricGeneratorModel, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Assuming pre-trained embeddings are loaded externally\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # Input size to LSTM is the size of word embedding + MIDI feature vector size\n",
    "        self.lstm = nn.LSTM(embedding_dim + midi_vector_size, hidden_dim, num_layers, batch_first=True)\n",
    "        \n",
    "        # The linear layer that maps from hidden state space to vocabulary space\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, word_inputs, midi_features, hidden):\n",
    "        # Embed word inputs: shapes (batch_size, seq_length) -> (batch_size, seq_length, embedding_dim)\n",
    "        embeds = self.word_embeddings(word_inputs)\n",
    "        \n",
    "        # Concatenate word embeddings and MIDI features along the feature dimension\n",
    "        # MIDI features should be replicated to match the sequence length of word inputs\n",
    "        midi_features = midi_features.unsqueeze(1).repeat(1, embeds.size(1), 1)\n",
    "        lstm_input = torch.cat((embeds, midi_features), 2)\n",
    "        \n",
    "        # LSTM output\n",
    "        lstm_out, hidden = self.lstm(lstm_input, hidden)\n",
    "        \n",
    "        # Final output layer\n",
    "        output = self.fc(lstm_out)\n",
    "        \n",
    "        # Softmax is applied externally if needed, e.g., nn.CrossEntropyLoss() does it internally\n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        # Initialize hidden and cell states with zeros\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.num_layers, batch_size, self.hidden_dim).zero_().to(weight.device),\n",
    "                  weight.new(self.num_layers, batch_size, self.hidden_dim).zero_().to(weight.device))\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LyricDataset(Dataset):\n",
    "    def __init__(self, lyrics_dict, midi_dict, word2vec):\n",
    "        \"\"\"\n",
    "        lyrics_dict: Dictionary of {song: [list of tokens]}\n",
    "        midi_dict: Dictionary of {song: midi_vector}\n",
    "        get_word_embedding: Function to convert a word to its corresponding embedding\n",
    "        \"\"\"\n",
    "        self.samples = []\n",
    "        for song, lyrics in lyrics_dict.items():\n",
    "            midi_vector = midi_dict[song]  # MIDI vector for the song\n",
    "            for i in range(len(lyrics) - 1):  # Exclude last word for which there is no next word\n",
    "                current_word_embedding = word2vec.get_vector(lyrics[i])\n",
    "                next_word_embedding = lyrics[i + 1]\n",
    "                self.samples.append((current_word_embedding, midi_vector, next_word_embedding))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        current_word_embedding, midi_vector, next_word_index = self.samples[idx]\n",
    "        return current_word_embedding, midi_vector, next_word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "logdeep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
