{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, embedding_weights, hidden_dim, vocab_size, num_layers=1):\n",
    "        \"\"\"\n",
    "        Initializes the LSTM model.\n",
    "        \n",
    "        Parameters:\n",
    "        - embedding_weights: Pre-trained Word2Vec embeddings.\n",
    "        - hidden_dim: The number of features in the hidden state `h` of the LSTM.\n",
    "        - vocab_size: The size of the vocabulary.\n",
    "        - num_layers: Number of recurrent layers (default=1).\n",
    "        \n",
    "        The input to the model is expected to be a batch of word indices,\n",
    "        and the output is a batch of predictions for the next word.\n",
    "        \"\"\"\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Embedding layer with pre-trained weights\n",
    "        self.word_embeddings = nn.Embedding.from_pretrained(embedding_weights, freeze=True)\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs and outputs hidden states\n",
    "        self.lstm = nn.LSTM(embedding_weights.shape[1], hidden_dim, num_layers, batch_first=True)\n",
    "\n",
    "        # The linear layer maps from hidden state space to vocabulary space\n",
    "        self.linear = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, input_word_indices):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the model.\n",
    "        \n",
    "        Parameters:\n",
    "        - input_word_indices: A batch of word indices as input.\n",
    "        \n",
    "        Returns:\n",
    "        - output: The model's predictions for the next word.\n",
    "        \"\"\"\n",
    "        embeddings = self.word_embeddings(input_word_indices)\n",
    "        lstm_out, _ = self.lstm(embeddings)\n",
    "        output = self.linear(lstm_out)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# # Load pre-trained Word2Vec embeddings\n",
    "# word2vec_path = 'models\\GoogleNews-vectors-negative300.bin.gz'  # Update this path\n",
    "# word2vec_model = KeyedVectors.load_word2vec_format(word2vec_path, binary=True)\n",
    "word2vec_path = \"models/word2vec-google-news-300.model\"\n",
    "word2vec_model = KeyedVectors.load(word2vec_path)\n",
    "\n",
    "# Prepare embedding weights in the format expected by PyTorch\n",
    "vocab_size = len(word2vec_model.key_to_index)\n",
    "embedding_dim = word2vec_model.vector_size\n",
    "embedding_weights = torch.zeros(vocab_size, embedding_dim)\n",
    "\n",
    "for word, idx in word2vec_model.key_to_index.items():\n",
    "    embedding_weights[idx] = torch.tensor(word2vec_model[word])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save embeddings model\n",
    "torch.save(embedding_weights, 'models/word2vec_weights.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting stoi and itos\n",
    "stoi = word2vec_model.key_to_index\n",
    "itos = word2vec_model.index_to_key\n",
    "\n",
    "# saving stoi and itos4\n",
    "import pickle\n",
    "with open('models/stoi.pkl', 'wb') as f:\n",
    "    pickle.dump(stoi, f)\n",
    "with open('models/itos.pkl', 'wb') as f:\n",
    "    pickle.dump(itos, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "class LyricsGenerator:\n",
    "    def __init__(self, model, vocab, device='cpu'):\n",
    "        \"\"\"\n",
    "        Initializes the LyricsGenerator.\n",
    "\n",
    "        Parameters:\n",
    "        - model: The trained LSTM model for lyrics generation.\n",
    "        - vocab: A mapping from words to indices and indices to words (vocab.stoi and vocab.itos).\n",
    "        - device: The device to run the generation on ('cpu' or 'cuda').\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.vocab = vocab\n",
    "        self.device = device\n",
    "\n",
    "    def sample_next_word(self, logits, temperature=1.0):\n",
    "        \"\"\"\n",
    "        Samples the next word from the logits with a given temperature.\n",
    "\n",
    "        Parameters:\n",
    "        - logits: The logits output by the model.\n",
    "        - temperature: Controls the randomness of the sampling. Higher values lead to more random outputs.\n",
    "\n",
    "        Returns:\n",
    "        - index of the sampled word.\n",
    "        \"\"\"\n",
    "        probabilities = F.softmax(logits / temperature, dim=-1)\n",
    "        word_index = torch.multinomial(probabilities, 1).item()\n",
    "        return word_index\n",
    "\n",
    "    def generate(self, start_word, max_words=50, max_words_per_line=10, temperature=1.0):\n",
    "        \"\"\"\n",
    "        Generates lyrics starting from a given word.\n",
    "\n",
    "        Parameters:\n",
    "        - start_word: The word to start generating from.\n",
    "        - max_words: The maximum number of words in the generated lyrics.\n",
    "        - max_words_per_line: The maximum number of words per line.\n",
    "        - temperature: Controls the randomness of the sampling.\n",
    "\n",
    "        Returns:\n",
    "        - A string containing the generated lyrics.\n",
    "        \"\"\"\n",
    "        self.model.eval()  # Set the model to evaluation mode\n",
    "        words = [start_word]\n",
    "        current_word_index = torch.tensor([self.vocab.stoi[start_word]], device=self.device)\n",
    "\n",
    "        for _ in range(max_words - 1):\n",
    "            with torch.no_grad():\n",
    "                logits = self.model(current_word_index.unsqueeze(0))[:, -1, :]\n",
    "                next_word_index = self.sample_next_word(logits, temperature)\n",
    "                next_word = self.vocab.itos[next_word_index]\n",
    "                words.append(next_word)\n",
    "                current_word_index = torch.tensor([next_word_index], device=self.device)\n",
    "\n",
    "                if len(words) % max_words_per_line == 0:\n",
    "                    words.append('\\n')\n",
    "\n",
    "            if words[-1] == '<eos>':  # Assuming <eos> is the end-of-sentence token\n",
    "                break\n",
    "\n",
    "        return ' '.join(words).replace(' \\n ', '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load stoi and itos\n",
    "with open('models/stoi.pkl', 'rb') as f:\n",
    "    stoi = pickle.load(f)\n",
    "with open('models/itos.pkl', 'rb') as f:\n",
    "    itos = pickle.load(f)\n",
    "\n",
    "# load pre-trained Word2Vec embeddings\n",
    "embedding_weights = torch.load('models/word2vec_weights.pt')\n",
    "vocab_size, embedding_dim = embedding_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    \"\"\"\n",
    "    A mapping from words to indices and indices to words.\n",
    "    \"\"\"\n",
    "    def __init__(self, stoi, itos):\n",
    "        self.stoi = stoi\n",
    "        self.itos = itos\n",
    "    \n",
    "    def __call__(self, word):\n",
    "        if word in self.stoi:\n",
    "            return self.stoi[word]\n",
    "        else:\n",
    "            return self.stoi['<unk>']\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.stoi)\n",
    "\n",
    "\n",
    "vocab = Vocabulary(stoi, itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the LSTM model with Word2Vec weights and corrected vocabulary size\n",
    "lstm_model = LSTMModel(embedding_weights=embedding_weights, hidden_dim=256, vocab_size=vocab_size, num_layers=1)\n",
    "lstm_model.to(device)\n",
    "\n",
    "# Instantiate the LyricsGenerator with the corrected mappings\n",
    "generator = LyricsGenerator(lstm_model, vocab, device)\n",
    "initial_word = 'the'  # Starting word for song generation\n",
    "song = generator.generate(initial_word)\n",
    "print(song)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary writer will output to ./runs/ directory by default\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Initialize TensorBoard writer\n",
    "writer = SummaryWriter('runs/lyrics_generator_experiment')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"epochs\": 100,\n",
    "    \"batch_size\": 64,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"hidden_dim\": 256,\n",
    "    \"num_layers\": 1,\n",
    "    \"embedding_dim\": 100,  # Assuming we know the embedding dimension\n",
    "    \"vocab_size\": len(vocab),  # Make sure this matches the actual vocab size\n",
    "    \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    \"batch_size\": 64,\n",
    "    \"shuffle\": False,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def train_model(model, train_loader, config):\n",
    "    \"\"\"\n",
    "    Trains the LSTM model on the given dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    - model: The LSTM model to train.\n",
    "    - train_loader: DataLoader for the training dataset.\n",
    "    - config: Dictionary containing configuration parameters.\n",
    "    \"\"\"\n",
    "    model.train()  # Switch model to training mode\n",
    "    optimizer = Adam(model.parameters(), lr=config[\"learning_rate\"])\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    model.to(config[\"device\"])\n",
    "    \n",
    "    for epoch in range(config[\"epochs\"]):\n",
    "        total_loss = 0\n",
    "        progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), leave=False)\n",
    "        for i, (input_words, target_words) in progress_bar:\n",
    "            input_words, target_words = input_words.to(config[\"device\"]), target_words.to(config[\"device\"])\n",
    "            \n",
    "            # Forward pass\n",
    "            output = model(input_words)\n",
    "            loss = criterion(output.view(-1, config[\"vocab_size\"]), target_words.view(-1))\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            progress_bar.set_description(f\"Epoch {epoch+1} Loss: {total_loss/(i+1):.4f}\")\n",
    "            \n",
    "        # Log the average loss for the epoch\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        writer.add_scalar('training_loss', avg_loss, epoch+1)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1} Completed. Avg Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model weights\n",
    "torch.save(lstm_model.state_dict(), 'models/lstm_model_weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.parsing.preprocessing import strip_punctuation, strip_numeric\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# nltk.download('punkt')  # Uncomment if nltk's punkt tokenizer hasn't been downloaded yet\n",
    "\n",
    "word2vec_path = 'models/GoogleNews-vectors-negative300.bin.gz'  # Adjust path as necessary\n",
    "word2vec_model = KeyedVectors.load_word2vec_format(word2vec_path, binary=True)\n",
    "\n",
    "bos_token = 'BOS'\n",
    "eos_token = 'EOS'\n",
    "eof_token = 'EOF'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_tokenize_lyrics(lyrics):\n",
    "    \"\"\"\n",
    "    Preprocesses and tokenizes lyrics, reduces OOV by leveraging Word2Vec's vocabulary.\n",
    "\n",
    "    Parameters:\n",
    "    lyrics (str): The raw lyrics as a single string.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing the list of word indices, the list of vectors, and the OOV rate.\n",
    "    \"\"\"\n",
    "    # Preprocess lyrics\n",
    "    lyrics = lyrics.lower().replace('&', eos_token)\n",
    "    lyrics = f'{bos_token} {lyrics} {eof_token}'\n",
    "    lyrics = strip_punctuation(lyrics)\n",
    "    lyrics = strip_numeric(lyrics)\n",
    "    lyrics = re.sub(r'\\(.*?\\)|\\[.*?\\]', '', lyrics)  # Remove text inside parentheses and brackets\n",
    "    lyrics = lyrics.split()\n",
    "\n",
    "    # Tokenization and vectorization\n",
    "    word_ids = [word2vec_model.key_to_index.get(word, word2vec_model.key_to_index.get('UNK')) for word in lyrics]\n",
    "    #vectors = [word2vec_model.get_vector(word, None) for word in lyrics if word in word2vec_model]\n",
    "    oov_rate = sum(1 for word in lyrics if word not in word2vec_model) / len(lyrics)\n",
    "\n",
    "    return word_ids, vectors, oov_rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_lyrics_to_dataset(csv_path):\n",
    "    \"\"\"\n",
    "    Parses the CSV containing lyrics into a structured dataset with minimal OOV.\n",
    "\n",
    "    Parameters:\n",
    "    csv_path (str): Path to the CSV file with lyrics.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary where keys are 'song_name artist' and values are word indices lists.\n",
    "    list: A list containing OOV rates for each song.\n",
    "    \"\"\"\n",
    "    train = pd.read_csv(csv_path)\n",
    "    train['artist'] = train['artist'].str.strip()\n",
    "    train['song'] = train['song'].str.strip()\n",
    "\n",
    "    lyrics_dict = {}\n",
    "    oov_rates = []\n",
    "\n",
    "    for i, row in tqdm(train.iterrows(), total=len(train)):\n",
    "        word_ids, vectors, oov_rate = preprocess_and_tokenize_lyrics(row['lyrics'])\n",
    "        key = f\"{row['song']} {row['artist']}\"\n",
    "        lyrics_dict[key] = word_ids\n",
    "        oov_rates.append(oov_rate)\n",
    "\n",
    "    return lyrics_dict, oov_rates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "604831731cc8492db5582a937f207f1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/600 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average OOV Rate: 6.94%\n"
     ]
    }
   ],
   "source": [
    "csv_path = 'data/lyrics_train_set2.csv'  # Update to your CSV path\n",
    "lyrics_dict, oov_rates = parse_lyrics_to_dataset(csv_path)\n",
    "\n",
    "# Analyze OOV Rates\n",
    "average_oov_rate = sum(oov_rates) / len(oov_rates)\n",
    "print(f\"Average OOV Rate: {average_oov_rate * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save lyrics dict\n",
    "import pickle\n",
    "with open('data/lyrics_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(lyrics_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "class LyricsDataset(Dataset):\n",
    "    def __init__(self, lyrics_dict):\n",
    "        \"\"\"\n",
    "        Initializes the dataset with preprocessed lyrics.\n",
    "        \n",
    "        Parameters:\n",
    "        lyrics_dict (dict): A dictionary where keys are 'song_name artist' and values are lists of word indices.\n",
    "        \"\"\"\n",
    "        self.lyrics_indices = [indices for indices in lyrics_dict.values()]\n",
    "        self.all_indices = [idx for sublist in self.lyrics_indices for idx in sublist]\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the total number of word indices in the dataset.\"\"\"\n",
    "        return len(self.all_indices) - 1  # Subtract 1 because we use a look-ahead of 1 for targets\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Returns a tuple (current_word_index, next_word_index) for training.\n",
    "        \n",
    "        Parameters:\n",
    "        index (int): The index of the current word.\n",
    "        \n",
    "        Returns:\n",
    "        tuple: A tuple of tensors (current_word_index, next_word_index).\n",
    "        \"\"\"\n",
    "        return (torch.tensor(self.all_indices[index], dtype=torch.long), \n",
    "                torch.tensor(self.all_indices[index + 1], dtype=torch.long))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset and dataloader\n",
    "lyrics_dataset = LyricsDataset(lyrics_dict)\n",
    "train_loader = DataLoader(lyrics_dataset, batch_size=config[\"batch_size\"], shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "train_model(lstm_model, train_loader, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model weights\n",
    "torch.save(lstm_model.state_dict(), 'models/lstm_model_weights_x.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMModel(\n",
       "  (word_embeddings): Embedding(3000000, 300)\n",
       "  (lstm): LSTM(300, 256, batch_first=True)\n",
       "  (linear): Linear(in_features=256, out_features=3000000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluatiing the model\n",
    "# load model weights\n",
    "lstm_model = LSTMModel(embedding_weights=embedding_weights, hidden_dim=256, vocab_size=vocab_size, num_layers=1)\n",
    "lstm_model.load_state_dict(torch.load('models/lstm_model_weights_x.pth'))\n",
    "lstm_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new instance of the LyricsGenerator\n",
    "vocab = Vocabulary(stoi, itos)\n",
    "generator = LyricsGenerator(lstm_model, vocab, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a song\n",
    "initial_word = 'BOS'  # Starting word for song generation\n",
    "song = generator.generate(initial_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from scipy.spatial.distance import cosine\n",
    "import numpy as np\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.similarities import WmdSimilarity\n",
    "\n",
    "# Load your pre-trained Word2Vec model\n",
    "word2vec_model = KeyedVectors.load_word2vec_format('models/GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
    "\n",
    "def calculate_similarity_metrics(text1, text2, word2vec_model):\n",
    "    \"\"\"\n",
    "    Calculates cosine similarity, WMD, and BLEU score between two texts.\n",
    "    \n",
    "    Parameters:\n",
    "    text1 (str): The first text.\n",
    "    text2 (str): The second text.\n",
    "    word2vec_model (gensim.models.KeyedVectors): Pre-trained Word2Vec model.\n",
    "    \n",
    "    Returns:\n",
    "    dict: A dictionary containing the cosine similarity, WMD, and BLEU score.\n",
    "    \"\"\"\n",
    "    # Tokenize and vectorize texts\n",
    "    tokens1 = text1.lower().split()\n",
    "    tokens2 = text2.lower().split()\n",
    "    \n",
    "    vectors1 = [word2vec_model[word] for word in tokens1 if word in word2vec_model]\n",
    "    vectors2 = [word2vec_model[word] for word in tokens2 if word in word2vec_model]\n",
    "    \n",
    "    # Cosine similarity\n",
    "    # Avoid division by zero and ensure valid vectors for cosine similarity calculation\n",
    "    if len(vectors1) > 0 and len(vectors2) > 0:\n",
    "        mean_vector1 = np.mean(vectors1, axis=0)\n",
    "        mean_vector2 = np.mean(vectors2, axis=0)\n",
    "        cosine_sim = 1 - cosine(mean_vector1, mean_vector2)\n",
    "    else:\n",
    "        cosine_sim = float('nan')\n",
    "    \n",
    "    # WMD\n",
    "    wmd = word2vec_model.wmdistance(tokens1, tokens2)\n",
    "    \n",
    "    # BLEU score\n",
    "    # Note: `sentence_bleu` expects a list of reference sentences, where each reference is tokenized\n",
    "    bleu_score = sentence_bleu([tokens1], tokens2)\n",
    "    \n",
    "    return {\n",
    "        'Cosine Similarity': cosine_sim,\n",
    "        'WMD': wmd,\n",
    "        'BLEU Score': bleu_score\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'goodbye norma jean & though i never knew you at all & you had the grace to hold yourself & while those around you crawled & they crawled out of the woodwork & and they whispered into your brain & they set you on the treadmill & and they made you change your name & and it seems to me you lived your life & like a candle in the wind & never knowing who to cling to & when the rain set in & and i would liked to have known you & but i was just a kid & your candle burned out long before & your legend ever did & loneliness was tough & the toughest role you ever played & hollywood created a superstar & and pain was the price you paid & even when you died & oh the press still hounded you & all the papers had to say & was that marilyn was found in the nude & and it seems to me you lived your life & like a candle in the wind & never knowing who to cling to & when the rain set in & and i would liked to have known you & but i was just a kid & your candle burned out long before & your legend ever did & goodbye norma jean & though i never knew you at all & you had the grace to hold yourself & while those around you crawled & goodbye norma jean & from the young man in the twenty second row & who sees you as something as more than sexual & more than just our marilyn monroe & and it seems to me you lived your life & like a candle in the wind & never knowing who to cling to & when the rain set in & and i would liked to have known you & but i was just a kid & your candle burned out long before & your legend ever did & the candle burned out long before & your legend ever did &'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# load trainset\n",
    "train = pd.read_csv('data/lyrics_train_set2.csv')\n",
    "\n",
    "# get lyrics of a song\n",
    "lyrics = train.loc[0, 'lyrics']\n",
    "lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate similarity metrics\n",
    "cos, bleu = calculate_similarity_metrics(lyrics, song)\n",
    "print(f\"Cosine Similarity: {cos:.4f}\")\n",
    "print(f\"BLEU Score: {bleu:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amaruy/.conda/envs/lyricmaker/lib/python3.10/site-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing Aaron_Neville_-_Tell_It_Like_It_Is.mid: data byte must be in range 0..127\n",
      "Error processing Beastie_Boys_-_Girls.mid: Could not decode key with 1 flats and mode 255\n",
      "Error processing Billy_Joel_-_Movin'_Out.mid: data byte must be in range 0..127\n",
      "Error processing Billy_Joel_-_Pressure.mid: data byte must be in range 0..127\n",
      "Error processing Brian_McKnight_-_On_The_Down_Low.mid: \n",
      "Error processing Dan_Fogelberg_-_Leader_of_the_Band.mid: Could not decode key with 4 flats and mode 255\n",
      "Error processing David_Bowie_-_Lazarus.mid: Could not decode key with 16 sharps and mode 1\n",
      "Error processing Ed_Sheeran_-_Thinking_Out_Loud_-_Violin.mid: too many values to unpack (expected 2)\n",
      "Error processing Eric_Clapton_-_wonderful_tonight_-_live_extnd_version_@jiji@.mid: too many values to unpack (expected 2)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pretty_midi\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "def extract_features(midi_path, max_length=2048):\n",
    "    \"\"\"\n",
    "    Extracts musical features from a MIDI file.\n",
    "\n",
    "    Parameters:\n",
    "    midi_path (str): Path to the MIDI file.\n",
    "    max_length (int): Maximum length for normalized pitch and velocity arrays.\n",
    "\n",
    "    Returns:\n",
    "    np.ndarray: A combined feature vector including normalized pitches, velocities, and mean chroma.\n",
    "    \"\"\"\n",
    "    midi_data = pretty_midi.PrettyMIDI(midi_path)\n",
    "\n",
    "    # Normalized Pitches\n",
    "    pitches = [note.pitch for instrument in midi_data.instruments for note in instrument.notes]\n",
    "    pitches_normalized = np.array(pitches) / 127.0  # MIDI pitch range\n",
    "    pitches_feature = np.zeros(max_length)\n",
    "    pitches_feature[:len(pitches_normalized)] = pitches_normalized[:max_length]\n",
    "\n",
    "    # Normalized Velocities\n",
    "    velocities = [note.velocity for instrument in midi_data.instruments for note in instrument.notes]\n",
    "    velocities_normalized = np.array(velocities) / 127.0  # MIDI velocity range\n",
    "    velocities_feature = np.zeros(max_length)\n",
    "    velocities_feature[:len(velocities_normalized)] = velocities_normalized[:max_length]\n",
    "\n",
    "    # Mean Chroma\n",
    "    chroma = midi_data.get_chroma()\n",
    "    chroma_mean = np.mean(chroma, axis=1)\n",
    "\n",
    "    # Combine features into a single vector\n",
    "    feature_vector = np.concatenate([pitches_feature, velocities_feature, chroma_mean])\n",
    "\n",
    "    return feature_vector\n",
    "\n",
    "def process_midi_files(directory, output_file):\n",
    "    \"\"\"\n",
    "    Processes all MIDI files in a directory, extracting features and saving them.\n",
    "\n",
    "    Parameters:\n",
    "    directory (str): Directory containing MIDI files.\n",
    "    output_file (str): File path to save the extracted feature vectors.\n",
    "    \"\"\"\n",
    "    feature_vectors = {}\n",
    "\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".mid\"):\n",
    "            midi_path = os.path.join(directory, filename)\n",
    "            try:\n",
    "                # Parsing artist and song names from the filename\n",
    "                artist, song = os.path.basename(filename).replace('.mid', '').lower().split('_-_')\n",
    "                song = f\"{song} {artist}\".replace('_', ' ')\n",
    "                feature_vector = extract_features(midi_path)\n",
    "                feature_vectors[song] = feature_vector\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {filename}: {e}\")\n",
    "\n",
    "    # Save the feature vectors to a file\n",
    "    with open(output_file, 'wb') as f:\n",
    "        pickle.dump(feature_vectors, f)\n",
    "\n",
    "# Example usage\n",
    "midi_directory = 'data/midi_files'\n",
    "output_file = 'midi_feature_vectors.pkl'\n",
    "process_midi_files(midi_directory, output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Processing MIDI Files (Outside the Code)\n",
    "\n",
    "Before feeding MIDI files into the feature extraction code, consider the following preprocessing steps to ensure the data is clean and consistent:\n",
    "\n",
    "2. **Normalization**: Ensure MIDI files are using a standard format and quantization to maintain consistency in timing and note representation.\n",
    "\n",
    "3. **Instrument Filtering**: Optionally, filter out specific instruments or tracks (e.g., drums) that may not contribute to the desired features.\n",
    "\n",
    "4. **Handling Polyphony**: Decide on how to handle polyphonic music, where multiple notes are played simultaneously. This could influence pitch and chroma feature extraction.\n",
    "\n",
    "\n",
    "### Process Overview for Training the Autoencoder with MIDI Data\n",
    "\n",
    "1. **Feature Extraction**: MIDI files are preprocessed to extract meaningful features, such as normalized pitches, velocities, and chroma mean. These features are stored in a dictionary, mapping song identifiers to feature vectors.\n",
    "\n",
    "2. **Dataset Preparation**: A custom `Dataset` class, `VectorDataset`, is created to\n",
    "\n",
    " handle the feature vectors. This class facilitates loading the data into a PyTorch `DataLoader` for efficient batch processing during training.\n",
    "\n",
    "3. **Autoencoder Architecture**: The autoencoder comprises an encoder and a decoder. The encoder compresses the input feature vectors into a smaller embedding, capturing the essential information. The decoder then attempts to reconstruct the original feature vector from this embedding.\n",
    "\n",
    "4. **Training Loop**: The autoencoder is trained using a mean squared error (MSE) loss function to minimize the difference between the original feature vectors and their reconstructions. The training process involves forward propagation to compute the loss, followed by backward propagation to update the model's weights.\n",
    "\n",
    "5. **Embedding Extraction**: After training, the encoder part of the autoencoder can be used independently to convert input feature vectors into compact embeddings. These embeddings will serve as inputs to the LSTM model for lyric generation, providing a musical context based on the MIDI data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import pickle\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_dim=1024):\n",
    "        \"\"\"\n",
    "        Initializes the Autoencoder model with a specified input size and embedding dimension.\n",
    "\n",
    "        Parameters:\n",
    "        - input_size (int): The size of the input feature vector.\n",
    "        - embedding_dim (int): The size of the embedding vector.\n",
    "        \"\"\"\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(4096, 2048),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(2048, embedding_dim),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, 2048),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(2048, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(4096, input_size),\n",
    "            nn.Sigmoid(),  # Sigmoid activation to ensure output values are between 0 and 1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the Autoencoder. Encodes and then decodes the input.\n",
    "\n",
    "        Parameters:\n",
    "        - x (torch.Tensor): Input tensor.\n",
    "\n",
    "        Returns:\n",
    "        - torch.Tensor: Reconstructed input tensor.\n",
    "        \"\"\"\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "    def encode(self, x):\n",
    "        \"\"\"\n",
    "        Encodes the input into a lower-dimensional embedding.\n",
    "\n",
    "        Parameters:\n",
    "        - x (torch.Tensor): Input tensor.\n",
    "\n",
    "        Returns:\n",
    "        - torch.Tensor: Encoded (embedded) tensor.\n",
    "        \"\"\"\n",
    "        return self.encoder(x)\n",
    "\n",
    "class VectorDataset(Dataset):\n",
    "    def __init__(self, vector_dict):\n",
    "        \"\"\"\n",
    "        Initializes the dataset with vectors extracted from MIDI files.\n",
    "\n",
    "        Parameters:\n",
    "        - vector_dict (dict): Dictionary containing feature vectors.\n",
    "        \"\"\"\n",
    "        self.vectors = list(vector_dict.values())\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the number of items in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.vectors)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieves an item by its index.\n",
    "\n",
    "        Parameters:\n",
    "        - idx (int): Index of the item.\n",
    "\n",
    "        Returns:\n",
    "        - torch.Tensor: Feature vector as a tensor.\n",
    "        \"\"\"\n",
    "        vector = self.vectors[idx]\n",
    "        return torch.tensor(vector, dtype=torch.float)\n",
    "\n",
    "def train_autoencoder(autoencoder, dataloader, criterion, optimizer, num_epochs=100, device=torch.device(\"cpu\")):\n",
    "    \"\"\"\n",
    "    Trains the autoencoder model.\n",
    "\n",
    "    Parameters:\n",
    "    - autoencoder (Autoencoder): The autoencoder model.\n",
    "    - dataloader (DataLoader): DataLoader for the dataset.\n",
    "    - criterion (torch.nn.modules.loss): Loss function.\n",
    "    - optimizer (torch.optim.Optimizer): Optimizer.\n",
    "    - num_epochs (int): Number of epochs to train.\n",
    "    - device (torch.device): Device to train on.\n",
    "    \"\"\"\n",
    "    # write to tensorboard\n",
    "    writer = SummaryWriter('runs/autoencoder_midi')\n",
    "    autoencoder.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for data in dataloader:\n",
    "            inputs = data.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = autoencoder(inputs)\n",
    "            loss = criterion(outputs, inputs)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # Log the average loss for the epoch\n",
    "        epoch_loss = running_loss / len(dataloader)\n",
    "        writer.add_scalar('training_loss', epoch_loss, epoch+1)\n",
    "        if (epoch+1) % 1000 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}\")\n",
    "   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the feature vectors\n",
    "with open('data/midi_feature_vectors.pkl', 'rb') as f:\n",
    "    feature_vectors = pickle.load(f)\n",
    "\n",
    "# Preparing dataset and dataloader\n",
    "dataset = VectorDataset(feature_vectors)\n",
    "dataloader = DataLoader(dataset, batch_size=2048, shuffle=True)\n",
    "\n",
    "# Model, loss, and optimizer setup\n",
    "input_size = len(next(iter(feature_vectors.values())))\n",
    "autoencoder = Autoencoder(input_size=input_size).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(autoencoder.parameters(), lr=1e-4)\n",
    "\n",
    "# Training the model\n",
    "train_autoencoder(autoencoder, dataloader, criterion, optimizer, num_epochs=1_000_000, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(autoencoder.state_dict(), 'models/midi_autoencoder_weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8974/1087672001.py:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:210.)\n",
      "  vector = torch.tensor([vector], dtype=torch.float).to(device)\n"
     ]
    }
   ],
   "source": [
    "# generate the embeddings\n",
    "autoencoder.eval()\n",
    "embeddings = {}\n",
    "for key, vector in feature_vectors.items():\n",
    "    vector = torch.tensor([vector], dtype=torch.float).to(device)\n",
    "    embedding = autoencoder.encoder(vector).detach().cpu().numpy()\n",
    "    embeddings[key] = embedding\n",
    "\n",
    "# save embeddings\n",
    "with open('data/midi_embeddings.pkl', 'wb') as f:\n",
    "    pickle.dump(embeddings, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amaruy/.conda/envs/lyricmaker/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, vocab_size, num_layers=1):\n",
    "        \"\"\"\n",
    "        Initializes the modified LSTM model to accept concatenated word and MIDI embeddings as input.\n",
    "\n",
    "        Parameters:\n",
    "        - input_dim (int): The dimensionality of the concatenated input vector (word embedding + MIDI embedding).\n",
    "        - hidden_dim (int): The number of features in the hidden state `h` of the LSTM.\n",
    "        - vocab_size (int): The size of the vocabulary, used for the output layer dimension.\n",
    "        - num_layers (int): Number of recurrent layers (default=1).\n",
    "        \n",
    "        The input to the model is expected to be a batch of concatenated word and MIDI embeddings,\n",
    "        and the output is a batch of predictions for the next word.\n",
    "        \"\"\"\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # LSTM layer takes concatenated embeddings as inputs\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "\n",
    "        # Linear layer that maps from hidden state space to vocabulary space\n",
    "        self.linear = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, concatenated_embeddings):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the model using concatenated word and MIDI embeddings.\n",
    "        \n",
    "        Parameters:\n",
    "        - concatenated_embeddings: A batch of concatenated word and MIDI embeddings.\n",
    "        \n",
    "        Returns:\n",
    "        - output: The model's predictions for the next word.\n",
    "        \"\"\"\n",
    "        lstm_out, _ = self.lstm(concatenated_embeddings)\n",
    "        output = self.linear(lstm_out)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from tqdm.auto import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, config):\n",
    "    \"\"\"\n",
    "    Trains the LSTM model on the given dataset, using preprocessed concatenated embeddings.\n",
    "\n",
    "    Parameters:\n",
    "    - model (torch.nn.Module): The LSTM model to be trained.\n",
    "    - train_loader (DataLoader): DataLoader for the training dataset, providing batches of preprocessed inputs and targets.\n",
    "    - config (dict): Configuration parameters including epochs, learning rate, device, etc.\n",
    "\n",
    "    The DataLoader is expected to yield batches of (input_features, target_words), where:\n",
    "    - input_features are the concatenated word and MIDI embeddings,\n",
    "    - target_words are the indices of the target words to predict.\n",
    "    \"\"\"\n",
    "    writer = SummaryWriter(f\"runs/{config['experiment_name']}\")\n",
    "    model.train()  # Ensure the model is in training mode\n",
    "    optimizer = Adam(model.parameters(), lr=config[\"learning_rate\"])\n",
    "    criterion = CrossEntropyLoss()  # Appropriate for classification tasks\n",
    "    model.to(config[\"device\"])  # Move model to configured device (CPU/GPU)\n",
    "\n",
    "    for epoch in range(config[\"epochs\"]):\n",
    "        total_loss = 0.0\n",
    "        # Enable or disable the progress bar based on verbosity setting\n",
    "        progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), leave=False, disable=not config.get(\"verbosity\", False))\n",
    "        \n",
    "        for i, (input_features, target_words) in progress_bar:\n",
    "            input_features, target_words = input_features.to(config[\"device\"]), target_words.to(config[\"device\"])\n",
    "            \n",
    "            optimizer.zero_grad()  # Clear gradients\n",
    "            outputs = model(input_features)  # Forward pass\n",
    "            loss = criterion(outputs.view(-1, config[\"vocab_size\"]), target_words.view(-1))  # Compute loss\n",
    "            loss.backward()  # Backward pass\n",
    "            optimizer.step()  # Update weights\n",
    "\n",
    "            total_loss += loss.item()  # Accumulate loss\n",
    "            progress_bar.set_description(f\"Epoch {epoch+1} Loss: {total_loss/(i+1):.4f}\")\n",
    "        \n",
    "        avg_loss = total_loss / len(train_loader)  # Calculate average loss\n",
    "        writer.add_scalar('training_loss', avg_loss, epoch+1)  # Log to TensorBoard\n",
    "        print(f\"Epoch {epoch+1} Completed. Avg Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lyrics embeddings saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from gensim.models import KeyedVectors\n",
    "import numpy as np\n",
    "\n",
    "# Load the pre-trained Word2Vec model\n",
    "# Make sure to adjust the path to where your Word2Vec model is stored\n",
    "word2vec_model_path = 'models/word2vec-google-news-300.model'\n",
    "word2vec_model = KeyedVectors.load(word2vec_model_path)\n",
    "\n",
    "# Load the lyrics dictionary\n",
    "with open('data/lyrics_dict.pkl', 'rb') as f:\n",
    "    lyrics_dict = pickle.load(f)\n",
    "\n",
    "# Initialize the dictionary to store embeddings\n",
    "lyrics_embeddings = {}\n",
    "\n",
    "# Convert word indices to embeddings\n",
    "for song_artist, word_ids in lyrics_dict.items():\n",
    "    # Convert each word id to its corresponding embedding\n",
    "    word_embeddings = []\n",
    "    for word_id in word_ids:\n",
    "        word = word2vec_model.index_to_key[word_id]  # Convert index to word\n",
    "        if word in word2vec_model:\n",
    "            word_embeddings.append(word2vec_model[word])\n",
    "        else:\n",
    "            print('.', end='')\n",
    "            # Handle out-of-vocabulary words\n",
    "            # Here, you can choose to skip or use a zero vector; let's use a zero vector\n",
    "            word_embeddings.append(np.zeros(word2vec_model.vector_size))\n",
    "\n",
    "    lyrics_embeddings[song_artist] = word_embeddings\n",
    "\n",
    "# Save the embeddings dictionary\n",
    "with open('data/lyrics_embeddings.pkl', 'wb') as f:\n",
    "    pickle.dump(lyrics_embeddings, f)\n",
    "\n",
    "print(\"Lyrics embeddings saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class LyricsMIDIDataset(Dataset):\n",
    "    def __init__(self, lyrics_dict=None, midi_embeddings=None, word2vec_model=None, preloaded_inputs=None, preloaded_targets=None):\n",
    "        \"\"\"\n",
    "        Initializes the dataset with either raw data to be processed or preloaded processed data.\n",
    "        \n",
    "        Parameters:\n",
    "        - lyrics_dict (dict): Raw lyrics data.\n",
    "        - midi_embeddings (dict): Raw MIDI embeddings.\n",
    "        - word2vec_model: The Word2Vec model.\n",
    "        - preloaded_inputs (torch.Tensor): Preloaded inputs tensor.\n",
    "        - preloaded_targets (torch.Tensor): Preloaded targets tensor.\n",
    "        \"\"\"\n",
    "        if preloaded_inputs is not None and preloaded_targets is not None:\n",
    "            self.inputs = preloaded_inputs\n",
    "            self.targets = preloaded_targets\n",
    "        else:\n",
    "            self.inputs = []\n",
    "            self.targets = []\n",
    "            # get all songs that arent in keys of midi_embeddings or lyrics_dict\n",
    "            self.missing_songs = set(lyrics_dict.keys()) ^ set(midi_embeddings.keys())\n",
    "\n",
    "            for song_key, word_indices in lyrics_dict.items():\n",
    "                if song_key not in midi_embeddings:\n",
    "                    continue  # Skip songs without a corresponding MIDI embedding\n",
    "                midi_embedding = midi_embeddings[song_key]  # MIDI embedding for the current song\n",
    "\n",
    "                for i in range(len(word_indices) - 1):\n",
    "                    # Convert word indices to embeddings\n",
    "                    word_embedding_current = word2vec_model[word_indices[i]]\n",
    "                    next_word_indice = word_indices[i + 1]\n",
    "\n",
    "                    # Concatenate word embedding with MIDI embedding for the input\n",
    "                    word_embedding_current = word_embedding_current.reshape(1, -1)  # Reshape to (1, embedding_dim)\n",
    "                    input_feature = np.concatenate([word_embedding_current, midi_embedding], axis=1)\n",
    "                    self.inputs.append(input_feature)\n",
    "                    self.targets.append(next_word_indice)\n",
    "\n",
    "            # Convert lists to tensors for PyTorch compatibility\n",
    "            self.inputs = torch.tensor(self.inputs, dtype=torch.float)\n",
    "            self.targets = torch.tensor(self.targets, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the total number of input-target pairs.\"\"\"\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns an input-target pair by index.\n",
    "        \n",
    "        Parameters:\n",
    "        - idx (int): The index of the input-target pair.\n",
    "        \n",
    "        Returns:\n",
    "        - tuple: A tuple containing the input feature tensor and target tensor.\n",
    "        \"\"\"\n",
    "        return self.inputs[idx], self.targets[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "word2vec_model_path = 'models/word2vec-google-news-300.model'  # Adjust this path\n",
    "word2vec_model = KeyedVectors.load(word2vec_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Adjust these paths to where your files are stored\n",
    "lyrics_dict_path = 'data/lyrics_dict.pkl'\n",
    "midi_embeddings_path = 'data/midi_embeddings.pkl'\n",
    "\n",
    "with open(lyrics_dict_path, 'rb') as f:\n",
    "    lyrics_dict = pickle.load(f)\n",
    "\n",
    "with open(midi_embeddings_path, 'rb') as f:\n",
    "    midi_embeddings = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"batch_size\": 8,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"hidden_dim\": 128,\n",
    "    \"num_layers\": 2,\n",
    "    \"embedding_dim\": 300,  # Assuming we know the embedding dimension\n",
    "    \"vocab_size\": 3000000,  # Make sure this matches the actual vocab size\n",
    "    \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    \"shuffle\": False,\n",
    "    \"epochs\": 10,\n",
    "    'verbosity': True\n",
    "}\n",
    "\n",
    "# clear cached memory if config['device'] is cuda\n",
    "if config['device'] == torch.device('cuda'):\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "config[\"experiment_name\"] = f\"lstm_{config['hidden_dim']}_{config['num_layers']}_{config['learning_rate']}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = LyricsMIDIDataset(lyrics_dict=lyrics_dict, midi_embeddings=midi_embeddings, word2vec_model=word2vec_model)\n",
    "\n",
    "torch.save(dataset.inputs, 'data/dataset_saved_inputs.pt')\n",
    "torch.save(dataset.targets, 'data/dataset_saved_targets.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6868/563711289.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  preloaded_targets = torch.tensor(preloaded_targets, dtype=torch.long)  # Ensure targets are long\n"
     ]
    }
   ],
   "source": [
    "preloaded_inputs = torch.load('data/dataset_saved_inputs.pt')\n",
    "preloaded_targets = torch.load('data/dataset_saved_targets.pt')\n",
    "preloaded_targets = torch.tensor(preloaded_targets, dtype=torch.long)  # Ensure targets are long\n",
    "\n",
    "# Initialize the dataset with the preloaded data\n",
    "dataset = LyricsMIDIDataset(preloaded_inputs=preloaded_inputs, preloaded_targets=preloaded_targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(dataset, batch_size=config['batch_size'], shuffle=config['shuffle'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1324"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the input dimension\n",
    "input_dim = dataset.inputs.shape[2]\n",
    "input_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model, using config to set the number of epochs, learning rate, etc.\n",
    "lstm_model = LSTMModel(input_dim=input_dim, hidden_dim=config['hidden_dim'], vocab_size=config['vocab_size'], num_layers=config['num_layers'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(lstm_model, train_loader, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nnenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
