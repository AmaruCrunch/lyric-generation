{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pickle as pkl\n",
    "from time import time\n",
    "import sys\n",
    "\n",
    "import pretty_midi\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.parsing.preprocessing import remove_stopwords, strip_punctuation\n",
    "import gensim.downloader as api\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from itertools import product\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LyricsMelodyDataset(Dataset):\n",
    "\n",
    "    def __init__(self, df, word2vec_model, segment_size=4, sequence_length=50, df_size=1000):\n",
    "        self.df = df[:df_size]\n",
    "        self.word2vec_model = word2vec_model\n",
    "        self.segment_size = segment_size\n",
    "        self.sequence_length = sequence_length\n",
    "        \n",
    "        self.inputs = []\n",
    "        self.melodies = []\n",
    "        self.targets = []\n",
    "        \n",
    "        self._create_sequences_and_melodies()\n",
    "        self._standardize_melodies()\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        text = text.lower()\n",
    "        text = remove_stopwords(text)\n",
    "        text = strip_punctuation(text)\n",
    "        return text\n",
    "\n",
    "    def text_to_vector(self, text):\n",
    "        words = self.preprocess_text(text).split()\n",
    "        indices = [self.word2vec_model.key_to_index[word] for word in words if word in self.word2vec_model.key_to_index]\n",
    "        vectors = [self.word2vec_model[word] for word in words if word in self.word2vec_model.key_to_index]\n",
    "        return np.array(vectors), indices\n",
    "\n",
    "\n",
    "    def _create_sequences_and_melodies(self):\n",
    "        for index, row in tqdm(self.df.iterrows(), total=len(self.df)):\n",
    "            lyrics_vectors, lyrics_indices = self.text_to_vector(row['lyrics'])\n",
    "            total_segments = int(np.ceil(len(lyrics_vectors) / self.segment_size))\n",
    "            melody_vectors = self.vectorize_midi_segments(row['midi'], total_segments)\n",
    "            \n",
    "            expanded_melody_vectors = np.repeat(melody_vectors, repeats=self.segment_size, axis=0)[:len(lyrics_vectors)]\n",
    "\n",
    "            for i in range(0, len(lyrics_vectors) - self.sequence_length, self.segment_size):\n",
    "                input_sequence = lyrics_vectors[i:i+self.sequence_length]\n",
    "                melody_sequence = expanded_melody_vectors[i:i+self.sequence_length]\n",
    "                \n",
    "                target_sequence = lyrics_indices[i+1:i+self.sequence_length+1]\n",
    "\n",
    "                self.inputs.append(input_sequence)\n",
    "                self.melodies.append(melody_sequence)\n",
    "                self.targets.append(target_sequence)\n",
    "            \n",
    "    def _standardize_melodies(self):\n",
    "        all_melodies_flat = np.vstack(self.melodies)\n",
    "        scaler = StandardScaler()\n",
    "        all_melodies_flat_standardized = scaler.fit_transform(all_melodies_flat)\n",
    "        \n",
    "        num_samples = len(self.melodies)\n",
    "        self.melodies = all_melodies_flat_standardized.reshape(num_samples, self.sequence_length, -1)\n",
    "\n",
    "    def segment_midi(self, midi_data, n_segments):\n",
    "        total_duration = midi_data.get_end_time()\n",
    "        segment_duration = total_duration / n_segments\n",
    "        segments = []\n",
    "\n",
    "        for i in range(n_segments):\n",
    "            start_time = i * segment_duration\n",
    "            end_time = start_time + segment_duration\n",
    "            segments.append((start_time, end_time))\n",
    "        \n",
    "        return segments\n",
    "\n",
    "    def extract_features_for_segment(self, midi_data, start_time, end_time, fs=100, pedal_threshold=64):\n",
    "        notes = []\n",
    "        for instrument in midi_data.instruments:\n",
    "            for note in instrument.notes:\n",
    "                if start_time <= note.start < end_time:\n",
    "                    notes.append(note)\n",
    "        \n",
    "        # Calculate average pitch and velocity, and total duration of notes in the segment\n",
    "        if notes:\n",
    "            average_pitch = np.mean([note.pitch for note in notes])\n",
    "            average_velocity = np.mean([note.velocity for note in notes])\n",
    "            total_duration_notes = sum([note.end - note.start for note in notes if note.start < end_time and note.end > start_time])\n",
    "        else:\n",
    "            average_pitch = 0\n",
    "            average_velocity = 0\n",
    "            total_duration_notes = 0\n",
    "\n",
    "        # Create one-hot vector for instrument presence\n",
    "        instrument_vector = np.zeros(128)  # Assuming General MIDI\n",
    "        for instrument in midi_data.instruments:\n",
    "            if any(start_time <= note.start < end_time for note in instrument.notes):\n",
    "                instrument_vector[instrument.program] = 1\n",
    "        \n",
    "        # Calculate chroma features for the segment\n",
    "        chroma = midi_data.get_chroma(fs=fs, times=np.arange(start_time, end_time, 1./fs), pedal_threshold=pedal_threshold)\n",
    "        average_chroma = np.mean(chroma, axis=1)  # Averaging chroma vectors over the segment\n",
    "\n",
    "        # Concatenate all features into a single vector\n",
    "        features = np.concatenate(([average_pitch, average_velocity, total_duration_notes], instrument_vector, average_chroma))\n",
    "        \n",
    "        return features\n",
    "\n",
    "    def vectorize_midi_segments(self, midi_data, n_segments):\n",
    "        segments = self.segment_midi(midi_data, n_segments)\n",
    "        feature_vectors = []\n",
    "\n",
    "        for start_time, end_time in segments:\n",
    "            features = self.extract_features_for_segment(midi_data, start_time, end_time)\n",
    "            feature_vectors.append(features)\n",
    "    \n",
    "        return np.array(feature_vectors)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "     \n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            torch.tensor(self.inputs[idx], dtype=torch.float),\n",
    "            torch.tensor(self.melodies[idx], dtype=torch.float),\n",
    "            torch.tensor(self.targets[idx], dtype=torch.long)  # Adjusted to torch.long\n",
    "    )\n",
    "\n",
    "class LyricsMelodyLSTM(nn.Module):\n",
    "    def __init__(self, text_input_dim, melody_input_dim, hidden_dim, output_dim, num_layers=2, dropout=0, flag=False):\n",
    "        super(LyricsMelodyLSTM, self).__init__()\n",
    "        self.flag = flag\n",
    "\n",
    "        if flag:\n",
    "            self.text_lstm = nn.LSTM(text_input_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout)\n",
    "            self.melody_lstm = nn.LSTM(melody_input_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout)\n",
    "            self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        else:\n",
    "            self.lstm = nn.LSTM(text_input_dim + melody_input_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout)\n",
    "            self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, text, melody, hidden=None):\n",
    "        if self.flag:\n",
    "            text_output, text_hidden = self.text_lstm(text)\n",
    "            melody_output, melody_hidden = self.melody_lstm(melody)\n",
    "            combined = torch.cat((text_output, melody_output), dim=2)\n",
    "        else:\n",
    "            combined_input = torch.cat((text, melody), dim=2)\n",
    "            combined_output, combined_hidden = self.lstm(combined_input, hidden)\n",
    "            combined = combined_output\n",
    "        \n",
    "        output = self.fc(combined)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "def train_model(model, train_dataloader, val_dataloader, segment, sequence_length, epochs=3, lr=2e-5, model_save_path='model'):\n",
    "    writer = SummaryWriter(model_save_path)\n",
    "    train_losses, val_losses, best_val_loss = [], [], float('inf')\n",
    "    criterion, optimizer = nn.CrossEntropyLoss(), optim.Adam(model.parameters(), lr=lr)\n",
    "    scaler, scheduler = GradScaler(), StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss, n_examples = 0.0, 0\n",
    "        for text, melody, targets in train_dataloader:\n",
    "            text, melody, targets = text.to(device), melody.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            with autocast():  # Mixed precision\n",
    "                outputs = model(text, melody)\n",
    "                loss = criterion(outputs.view(-1, outputs.shape[-1]), targets.view(-1))\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            running_loss += loss.item() * text.size(0)\n",
    "            n_examples += text.size(0)\n",
    "        \n",
    "        scheduler.step()\n",
    "        avg_train_loss = running_loss / n_examples\n",
    "        val_loss, val_bleu = evaluate_model(model, val_dataloader, criterion)\n",
    "        \n",
    "        writer.add_scalar('Loss/train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/val', val_loss, epoch)\n",
    "        writer.add_scalar('BLEU/val', val_bleu, epoch)\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), f'{model_save_path}_{segment}_{sequence_length}.pth')\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Segment {segment}, Seq Length {sequence_length}, Train Loss: {avg_train_loss:.4f}, Val Loss: {val_loss:.4f}, Val BLEU: {val_bleu:.4f}')\n",
    "    \n",
    "    writer.close()\n",
    "    return train_losses, val_losses\n",
    "\n",
    "def evaluate_model(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_targets = []\n",
    "    all_outputs = []\n",
    "    with torch.no_grad():\n",
    "        for i, (text, melody, targets) in enumerate(dataloader):\n",
    "            text, melody, targets = text.to(device), melody.to(device), targets.to(device)\n",
    "            outputs = model(text, melody)\n",
    "            targets_flat = targets.view(-1)\n",
    "            outputs_flat = outputs.view(-1, outputs.shape[-1])\n",
    "            loss = criterion(outputs_flat, targets_flat.long())\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Prepare data for BLEU score calculation\n",
    "            targets_list = targets_flat.cpu().numpy().tolist()\n",
    "            outputs_indices = outputs_flat.argmax(dim=1).cpu().numpy().tolist()\n",
    "            all_targets.append([targets_list])  # BLEU expects a list of reference translations\n",
    "            all_outputs.append(outputs_indices)\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    bleu_score = corpus_bleu(all_targets, all_outputs)\n",
    "    return avg_loss, bleu_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running search 1 with segment size 1 and sequence length 100\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    'epochs': 30,\n",
    "    'batch_size': 248,\n",
    "    'learning_rate': 0.001,\n",
    "    'text_input_dim': 300,\n",
    "    'melody_input_dim': 143,\n",
    "    'hidden_dim': 256,\n",
    "    'output_dim': None,  # To be set after loading Word2Vec model\n",
    "    'num_layers': 2,\n",
    "    'dropout': 0.0,\n",
    "    'model_save_path': None,  # To be set dynamically based on experiment\n",
    "    'results_path': 'results/results.csv',\n",
    "    'train_dataset_path': None,  # To be set dynamically\n",
    "    'val_dataset_path': None,  # To be set dynamically\n",
    "    'test_dataset_path': None,  # To be set dynamically\n",
    "}\n",
    "\n",
    "# Load Word2Vec model and update config\n",
    "word2vec_path = 'word2vec-google-news-300.kv'\n",
    "word2vec_model = KeyedVectors.load(word2vec_path)\n",
    "config['output_dim'] = len(word2vec_model)\n",
    "\n",
    "# Determine experiment parameters based on search index\n",
    "search = 1\n",
    "segments = [1, 4, 8]\n",
    "sequence_lengths = [50, 100, 200]\n",
    "segment, sequence_length = list(product(segments, sequence_lengths))[search]\n",
    "\n",
    "# Update config with dynamic parameters\n",
    "config['model_save_path'] = f'seg{segment}_seq{sequence_length}'\n",
    "data_set_name = f'seg{segment}_seq{sequence_length}.pkl'\n",
    "config['train_dataset_path'] = 'data' + data_set_name\n",
    "config['val_dataset_path'] = 'data' + data_set_name\n",
    "config['test_dataset_path'] = 'data' + data_set_name\n",
    "\n",
    "print(f'Running search {search} with segment size {segment} and sequence length {sequence_length}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "logdeep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
